<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Human Robot Interaction(2021) · Armine's Page</title><meta name="description" content="Human Robot Interaction(2021) - Armine"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/images/bitbug_favicon.ico"><link rel="stylesheet" href="/css/prontera.css"><link rel="search" type="application/opensearchdescription+xml" href="https://ar-mine.github.io/atom.xml" title="Armine's Page"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Armine's Page" type="application/atom+xml">
</head><body><header class="feature-header"><nav class="component-nav"><ul><div class="logo-container"><a href="/"><h2 class="title">Armine's Page</h2></a></div><a href="/" target="_self" class="li component-nav-item"><p>INDEX</p></a><a href="/archives" target="_self" class="li component-nav-item"><p>ARCHIVES</p></a><a href="/publication" target="_self" class="li component-nav-item"><p>PUBLICATION</p></a><a href="/project" target="_self" class="li component-nav-item"><p>PROJECT</p></a><ul class="shortcut-icons"><a href="https://github.com/ar-mine" target="_blank"><img src="/images/github.svg" class="icon"></a></ul></ul></nav></header><main class="container"><div id="post-container"><div class="post"><article class="post-block"><h1 class="post-title">Human Robot Interaction(2021)</h1><div class="post-info">Apr 1, 2021</div><div class="post-content"><h1 id="Human-Collision-Avoidance-based-on-Global-Skeleton-Detection"><a href="#Human-Collision-Avoidance-based-on-Global-Skeleton-Detection" class="headerlink" title="Human Collision Avoidance based on Global Skeleton Detection"></a>Human Collision Avoidance based on Global Skeleton Detection</h1><p>The quick glance(left) is a demo for exploring collision avoidance between humans and manipulators through global skeleton detection.(The camera is not shown in the animation, but you can see it in the picture(right) of workspace setup.)</p>
<p>We use <a target="_blank" rel="noopener" href="https://nuitrack.com/">Nuitrack</a> to detect skeleton keypoints of human and then model these points as spheres which are given collision volume so that <a target="_blank" rel="noopener" href="https://moveit.ros.org/">Moveit!</a> can plan a trajectory to avoid collision. If there is any potential collision appearing in the trajectory, the manipulator will stop first and replan a new path so that it can get around the obstacles.</p>
<div style="padding:5px; float:left">
<img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/human-collision-skeleton-experiment.gif" height=310px/>
</div>
<div style="padding:5px; float:right">
<img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/human-collision-skeleton-setup.jpg" height=310px/>
</div>
<span id="more"></span>
The attached link is for the full video(without speeding up).
<iframe src="https://www.youtube.com/embed/Sf_YErWoBm4" allowfullscreen frameborder="0" height="310," width="620," allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe>
<hr style="height:3px;background: #333"/>
<br/>

<h1 id="GGCNN-guided-Grasping-on-the-Plane-and-for-Handover"><a href="#GGCNN-guided-Grasping-on-the-Plane-and-for-Handover" class="headerlink" title="GGCNN guided Grasping on the Plane and for Handover"></a>GGCNN guided Grasping on the Plane and for Handover</h1><div style="padding:2px; float:left">
<img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/GGCNN%20plan%20pratical%20grasp.gif" width=300px/>
</div>
<div style="padding:2px; float:right;">
<img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/handover%20stable.gif" width=300px/>
</div>

<p>The first quick glance is a demo grasping objects fixed on the plane and the second one shows a human-to-robot handover task which means that robot tries to grasp from human hand while avoiding to collide with people meanwhile. Both two demos are based on <a target="_blank" rel="noopener" href="https://github.com/dougsm/ggcnn">GGCNN</a>, a light-weight network structure that can give pose candidates for grasping. </p>
<p>The attached link is for the full video of two experiments(without speeding up).</p>
<iframe src="https://www.youtube.com/embed/w83TNKI83qw" allowfullscreen frameborder="0" height="310," width="620," allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe>
<hr style="height:3px;background: #333"/>
<br/>

<h1 id="PBVS-tracking-for-marker-and-hand"><a href="#PBVS-tracking-for-marker-and-hand" class="headerlink" title="PBVS tracking for marker and hand"></a>PBVS tracking for marker and hand</h1><div style="padding:2px; float:left">
<img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/PBVS%20tracking%20for%20marker.gif" width=300px/>
</div>
<div style="padding:2px; float:right;">
<img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/PBVS-tracking-for-hand.gif" width=300px/>
</div>

<p>These two demos are to implement servoing interface for UR3e robot based on (Position based Visual Servoing)PBVS instead of using Moveit. The first demo uses Acuro makers as target pose while the second one track hand which is processed by <a target="_blank" rel="noopener" href="https://github.com/CoinCheung/BiSeNet">BiSeNet</a>. To use the network in my task, I train it with <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.05446">LIP</a> and <a target="_blank" rel="noopener" href="http://vision.soic.indiana.edu/projects/egohands/">Egohand</a> dataset by myself.</p>
<p>The attached link is for the full video of two experiments(without speeding up).</p>
<iframe src="https://www.youtube.com/embed/BBpfJdAknTI" allowfullscreen frameborder="0" height="310," width="620," allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe>

</div></article></div></div></main><footer class="footer-container"><div class="paginator"><a href="/2022/01/27/human-robot-interaction2/" class="prev">PREV</a><a href="/2018/05/10/hexapod-robot/" class="next">NEXT</a></div><div class="copyright"><p>© 2022 <a href="https://ar-mine.github.io">Armine</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/AngryPowman/hexo-theme-prontera" target="_blank">hexo-theme-prontera</a>.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"armine",'auto');ga('send','pageview');</script></body></html>