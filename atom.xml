<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Armine&#39;s Page</title>
  
  
  <link href="https://ar-mine.github.io/atom.xml" rel="self"/>
  
  <link href="https://ar-mine.github.io/"/>
  <updated>2022-01-25T04:20:00.000Z</updated>
  <id>https://ar-mine.github.io/</id>
  
  <author>
    <name>Armine</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>About Me</title>
    <link href="https://ar-mine.github.io/2022/09/23/about%20me/"/>
    <id>https://ar-mine.github.io/2022/09/23/about%20me/</id>
    <published>2022-09-23T15:22:32.217Z</published>
    <updated>2022-01-25T04:20:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>(This page is not adapted for mobile phones, so please try to browse it on the PC if you can.)</strong></p><p>I am a Research associate in <a href="https://lvchen.wixsite.com/automan">AutomMan Lab</a> working with Asst Prof. Chen Lv at <a href="https://www.ntu.edu.sg/">MAE, Nanyang Technology University (NTU), Singapore</a> now. I graduated from NTU and got my MSc. degree in Computer Control and Automation in Apr 2022 (Dissertation Supervisor: <a href="https://personal.ntu.edu.sg/gqhu/index.html">Prof. GuoQiang Hu</a>). Before that, I received my B.Eng in Automation from <a href="https://www.hfut.edu.cn/">Hefei University of Technology (HFUT), China</a> in Jun 2020. At undergraduate years, I tried to explore and conducted a preliminary look to some fields, such as <a href="/2018/01/25/teleoperated-car-based-on-EEG/" title="Teleoperated Car based on EEG control with Video Signal Stimulation">electroencephalography(EEG) interaction</a>, <a href="/2018/05/10/hexapod-robot/" title="Hexapod Robot(2018)">hexopod robot</a>, <a href="/2020/01/25/semantic-segmentation/" title="semantic-segmentation">computer vision</a> and <a href="https://ar-mine.github.io/publication/">reinforcement learning</a>. </p><p>Currently, I am responsible for manipulator part of the project in <a href="https://www.ntu.edu.sg/continental-ntu">Continental-NTU Corporate Lab</a> whose goal is industrial application of cobot and aAGV. And the topic of master dissertation was researching Human-Robot-Interaction(HRI) applied on the manipulator, such as <a href="/2021/04/01/human-robot-interaction/" title="Human Robot Interaction(2021)">Human-to-Robot(H2R)</a> and <a href="/2022/01/27/human-robot-interaction2/" title="Human Robot Interaction(2022)">collision avoidance based on proximity sensors</a>. Now I am finding a Ph.D. position and want to research more about intelligence and its pratical application. If you are interested in me, please feel free to contact me.</p><p><strong>Research interests</strong>: computer vision, robotics, human robot interaction and reinforcement learning.</p><span id="more"></span><!--[My CV](https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/RUNJIA_TAN-CV.pdf)-->]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;(This page is not adapted for mobile phones, so please try to browse it on the PC if you can.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I am a Research associate in &lt;a href=&quot;https://lvchen.wixsite.com/automan&quot;&gt;AutomMan Lab&lt;/a&gt; working with Asst Prof. Chen Lv at &lt;a href=&quot;https://www.ntu.edu.sg/&quot;&gt;MAE, Nanyang Technology University (NTU), Singapore&lt;/a&gt; now. I graduated from NTU and got my MSc. degree in Computer Control and Automation in Apr 2022 (Dissertation Supervisor: &lt;a href=&quot;https://personal.ntu.edu.sg/gqhu/index.html&quot;&gt;Prof. GuoQiang Hu&lt;/a&gt;). Before that, I received my B.Eng in Automation from &lt;a href=&quot;https://www.hfut.edu.cn/&quot;&gt;Hefei University of Technology (HFUT), China&lt;/a&gt; in Jun 2020. At undergraduate years, I tried to explore and conducted a preliminary look to some fields, such as &lt;a href=&quot;/2018/01/25/teleoperated-car-based-on-EEG/&quot; title=&quot;Teleoperated Car based on EEG control with Video Signal Stimulation&quot;&gt;electroencephalography(EEG) interaction&lt;/a&gt;, &lt;a href=&quot;/2018/05/10/hexapod-robot/&quot; title=&quot;Hexapod Robot(2018)&quot;&gt;hexopod robot&lt;/a&gt;, &lt;a href=&quot;/2020/01/25/semantic-segmentation/&quot; title=&quot;semantic-segmentation&quot;&gt;computer vision&lt;/a&gt; and &lt;a href=&quot;https://ar-mine.github.io/publication/&quot;&gt;reinforcement learning&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Currently, I am responsible for manipulator part of the project in &lt;a href=&quot;https://www.ntu.edu.sg/continental-ntu&quot;&gt;Continental-NTU Corporate Lab&lt;/a&gt; whose goal is industrial application of cobot and aAGV. And the topic of master dissertation was researching Human-Robot-Interaction(HRI) applied on the manipulator, such as &lt;a href=&quot;/2021/04/01/human-robot-interaction/&quot; title=&quot;Human Robot Interaction(2021)&quot;&gt;Human-to-Robot(H2R)&lt;/a&gt; and &lt;a href=&quot;/2022/01/27/human-robot-interaction2/&quot; title=&quot;Human Robot Interaction(2022)&quot;&gt;collision avoidance based on proximity sensors&lt;/a&gt;. Now I am finding a Ph.D. position and want to research more about intelligence and its pratical application. If you are interested in me, please feel free to contact me.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research interests&lt;/strong&gt;: computer vision, robotics, human robot interaction and reinforcement learning.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Human Robot Interaction(2022)</title>
    <link href="https://ar-mine.github.io/2022/01/27/human-robot-interaction2/"/>
    <id>https://ar-mine.github.io/2022/01/27/human-robot-interaction2/</id>
    <published>2022-01-27T03:47:52.000Z</published>
    <updated>2022-11-22T05:49:15.005Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Slow-Down-or-Stop-by-using-Proximity-Sensor"><a href="#Slow-Down-or-Stop-by-using-Proximity-Sensor" class="headerlink" title="Slow Down or Stop by using Proximity Sensor"></a>Slow Down or Stop by using Proximity Sensor</h1><br/><div style="padding:5px; float:left"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/task1-real.gif" width=260px/></div><p>According to the distance to collision, the robot can do some reactions like slowing down and even stop if it is close enough. To avoid the influence of the ray shadowed by itself, I code with <a href="https://github.com/flexible-collision-library/fcl">FCL</a> and implement the visualization of collision and self-collision check between robot body and sensors’ ray.</p><br/><span id="more"></span><hr style="height:3px;background: #333"/><br/><h1 id="Collision-Avoidance-based-on-the-Admittance-Control"><a href="#Collision-Avoidance-based-on-the-Admittance-Control" class="headerlink" title="Collision Avoidance based on the Admittance Control"></a>Collision Avoidance based on the Admittance Control</h1><p>I model the distance to collision as force imposed on the end effector of robot arm and implement an admittance controller to make it avoid collision.</p><div style="padding:5px; float:left"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/task2-real.gif" height=165px/></div><div style="padding:5px; float:left"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/task2-rviz.gif" height=165px/></div><p>The attached player is for the full video for above two demos.</p><iframe src="https://www.youtube.com/embed/-aYnKDmrGJ8" allowfullscreen frameborder="0" height="310," width="620," allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Slow-Down-or-Stop-by-using-Proximity-Sensor&quot;&gt;&lt;a href=&quot;#Slow-Down-or-Stop-by-using-Proximity-Sensor&quot; class=&quot;headerlink&quot; title=&quot;Slow Down or Stop by using Proximity Sensor&quot;&gt;&lt;/a&gt;Slow Down or Stop by using Proximity Sensor&lt;/h1&gt;&lt;br/&gt;
&lt;div style=&quot;padding:5px; float:left&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/task1-real.gif&quot; width=260px/&gt;
&lt;/div&gt;

&lt;p&gt;According to the distance to collision, the robot can do some reactions like slowing down and even stop if it is close enough. To avoid the influence of the ray shadowed by itself, I code with &lt;a href=&quot;https://github.com/flexible-collision-library/fcl&quot;&gt;FCL&lt;/a&gt; and implement the visualization of collision and self-collision check between robot body and sensors’ ray.&lt;/p&gt;
&lt;br/&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Human Robot Interaction(2021)</title>
    <link href="https://ar-mine.github.io/2021/04/01/human-robot-interaction/"/>
    <id>https://ar-mine.github.io/2021/04/01/human-robot-interaction/</id>
    <published>2021-04-01T06:30:08.000Z</published>
    <updated>2022-11-22T05:54:13.387Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Human-Collision-Avoidance-based-on-Global-Skeleton-Detection"><a href="#Human-Collision-Avoidance-based-on-Global-Skeleton-Detection" class="headerlink" title="Human Collision Avoidance based on Global Skeleton Detection"></a>Human Collision Avoidance based on Global Skeleton Detection</h1><p>The quick glance(left) is a demo for exploring collision avoidance between humans and manipulators through global skeleton detection.(The camera is not shown in the animation, but you can see it in the picture(right) of workspace setup.)</p><p>We use <a href="https://nuitrack.com/">Nuitrack</a> to detect skeleton keypoints of human and then model these points as spheres which are given collision volume so that <a href="https://moveit.ros.org/">Moveit!</a> can plan a trajectory to avoid collision. If there is any potential collision appearing in the trajectory, the manipulator will stop first and replan a new path so that it can get around the obstacles.</p><div style="padding:5px; float:left"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/human-collision-skeleton-experiment.gif" height=310px/></div><div style="padding:5px; float:right"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/human-collision-skeleton-setup.jpg" height=310px/></div><span id="more"></span>The attached link is for the full video(without speeding up).<iframe src="https://www.youtube.com/embed/Sf_YErWoBm4" allowfullscreen frameborder="0" height="310," width="620," allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe><hr style="height:3px;background: #333"/><br/><h1 id="GGCNN-guided-Grasping-on-the-Plane-and-for-Handover"><a href="#GGCNN-guided-Grasping-on-the-Plane-and-for-Handover" class="headerlink" title="GGCNN guided Grasping on the Plane and for Handover"></a>GGCNN guided Grasping on the Plane and for Handover</h1><div style="padding:2px; float:left"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/GGCNN%20plan%20pratical%20grasp.gif" width=300px/></div><div style="padding:2px; float:right;"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/handover%20stable.gif" width=300px/></div><p>The first quick glance is a demo grasping objects fixed on the plane and the second one shows a human-to-robot handover task which means that robot tries to grasp from human hand while avoiding to collide with people meanwhile. Both two demos are based on <a href="https://github.com/dougsm/ggcnn">GGCNN</a>, a light-weight network structure that can give pose candidates for grasping. </p><p>The attached link is for the full video of two experiments(without speeding up).</p><iframe src="https://www.youtube.com/embed/w83TNKI83qw" allowfullscreen frameborder="0" height="310," width="620," allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe><hr style="height:3px;background: #333"/><br/><h1 id="PBVS-tracking-for-marker-and-hand"><a href="#PBVS-tracking-for-marker-and-hand" class="headerlink" title="PBVS tracking for marker and hand"></a>PBVS tracking for marker and hand</h1><div style="padding:2px; float:left"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/PBVS%20tracking%20for%20marker.gif" width=300px/></div><div style="padding:2px; float:right;"><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/PBVS-tracking-for-hand.gif" width=300px/></div><p>These two demos are to implement servoing interface for UR3e robot based on (Position based Visual Servoing)PBVS instead of using Moveit. The first demo uses Acuro makers as target pose while the second one track hand which is processed by <a href="https://github.com/CoinCheung/BiSeNet">BiSeNet</a>. To use the network in my task, I train it with <a href="https://arxiv.org/abs/1703.05446">LIP</a> and <a href="http://vision.soic.indiana.edu/projects/egohands/">Egohand</a> dataset by myself.</p><p>The attached link is for the full video of two experiments(without speeding up).</p><iframe src="https://www.youtube.com/embed/BBpfJdAknTI" allowfullscreen frameborder="0" height="310," width="620," allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Human-Collision-Avoidance-based-on-Global-Skeleton-Detection&quot;&gt;&lt;a href=&quot;#Human-Collision-Avoidance-based-on-Global-Skeleton-Detection&quot; class=&quot;headerlink&quot; title=&quot;Human Collision Avoidance based on Global Skeleton Detection&quot;&gt;&lt;/a&gt;Human Collision Avoidance based on Global Skeleton Detection&lt;/h1&gt;&lt;p&gt;The quick glance(left) is a demo for exploring collision avoidance between humans and manipulators through global skeleton detection.(The camera is not shown in the animation, but you can see it in the picture(right) of workspace setup.)&lt;/p&gt;
&lt;p&gt;We use &lt;a href=&quot;https://nuitrack.com/&quot;&gt;Nuitrack&lt;/a&gt; to detect skeleton keypoints of human and then model these points as spheres which are given collision volume so that &lt;a href=&quot;https://moveit.ros.org/&quot;&gt;Moveit!&lt;/a&gt; can plan a trajectory to avoid collision. If there is any potential collision appearing in the trajectory, the manipulator will stop first and replan a new path so that it can get around the obstacles.&lt;/p&gt;
&lt;div style=&quot;padding:5px; float:left&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/human-collision-skeleton-experiment.gif&quot; height=310px/&gt;
&lt;/div&gt;
&lt;div style=&quot;padding:5px; float:right&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/human-collision-skeleton-setup.jpg&quot; height=310px/&gt;
&lt;/div&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Hexapod Robot(2018)</title>
    <link href="https://ar-mine.github.io/2018/05/10/hexapod-robot/"/>
    <id>https://ar-mine.github.io/2018/05/10/hexapod-robot/</id>
    <published>2018-05-10T04:48:02.000Z</published>
    <updated>2022-11-22T05:49:28.282Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The hexapod robot uses a master-slave control system with a high-performance MCU acting as the master who schedules the resources of each slave chip and takes on the computationally intensive work. The slave processor is mainly responsible for the control of the individual motion structures, thus reducing the load of the master.</p><p>The hexapod robot aims to search and rescue problems in complex and narrow environments. Therefore, we equipped it with Lidar and camera, and try to use ToF camera to improve the efficiency and accuracy of detection.</p><p>In the 1-year project, we implement the functions as below:</p><ol><li>The implementation of simple walking function with multiple processors, and capability of performing the corresponding basic movements according to the instructions of the remote control.</li><li>The implementation of obstacle avoidance with multi-modal perception(foot pressure, gyroscope, point cloud and RGB image).</li><li>The algorithm for walking over uneven surface.<span id="more"></span><h1 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h1><h2 id="Mechanical-modelling-design"><a href="#Mechanical-modelling-design" class="headerlink" title="Mechanical modelling design"></a>Mechanical modelling design</h2>The image below is the 3D model of our hexapod robot which is rendered by <code>Solidworks</code> and we then ordered a set of customized aluminium alloy boards to balance rigidity and lightness.<img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/hexopod-3d-model.png"/>Due to the large amount of servo motors to be used(6*3=18!), we chose to use the bus servo to reduce the labour costed on controlling each one. Howerver, we underestimated the weight of total robot and the torque of the servos cannot afford it stablly, which means the robot can only move in a slow speed(brushless motors maybe better but are difficult for us at that time).![](https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/hexapod-basic-assembly.jpg)<center>Basic hexapod and some deprecated modules</center>To minimize the rest of weight, we change the large capacity Lithium battery to small one. But in this way we need to charge more frequently :(</li></ol><h2 id="Processors"><a href="#Processors" class="headerlink" title="Processors"></a>Processors</h2><p>The master processor in the project is <code>Raspberry 3B+</code> while the slave one is <code>STM32 MCU</code>. The reason using the former one is that it is the most popular embedded PC with good computational power and the largest community so that we can get much infomation and materials about it. The later one is the most common type of MCU we used in the electrical design competition, so we use it. </p><p>The Raspberry is used to process the RGB, point cloud or depth data while the STM32 work with pressure sensors and need to parse the command from master and send specfic control message by <code>Socket</code>. In order to coordinate the different parts of hardware, we installed <code>ROS kinetic</code> on Raspberry and created interface for STM32 part based on <code>ros control</code> as motion controller.</p><h2 id="Sensors"><a href="#Sensors" class="headerlink" title="Sensors"></a>Sensors</h2><p>The sensors shown below were used in the project(the ToF camera was only tried).</p><center><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/hexapod-lidar.png"/></center><center>2-D lidar(due to limited funding)</center><center><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/hexapod-tof-camera.png"/></center><center>ToF camera(TI test version)</center><h2 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h2><h1 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h1><ul><li>Provincial Innovation and Entrepreneurship Training Program for College Students: Excellent</li><li>Third Prize of the 12th iCAN International Contest of innovAtioN, 2018<br><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/hexapod-ican-country.png"><br><img src="https://raw.githubusercontent.com/ar-mine/ar-mine.github.io/images/hexopod-ican.jpg"><center>Thanks for my team members! :)</center></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;The hexapod robot uses a master-slave control system with a high-performance MCU acting as the master who schedules the resources of each slave chip and takes on the computationally intensive work. The slave processor is mainly responsible for the control of the individual motion structures, thus reducing the load of the master.&lt;/p&gt;
&lt;p&gt;The hexapod robot aims to search and rescue problems in complex and narrow environments. Therefore, we equipped it with Lidar and camera, and try to use ToF camera to improve the efficiency and accuracy of detection.&lt;/p&gt;
&lt;p&gt;In the 1-year project, we implement the functions as below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The implementation of simple walking function with multiple processors, and capability of performing the corresponding basic movements according to the instructions of the remote control.&lt;/li&gt;
&lt;li&gt;The implementation of obstacle avoidance with multi-modal perception(foot pressure, gyroscope, point cloud and RGB image).&lt;/li&gt;
&lt;li&gt;The algorithm for walking over uneven surface.</summary>
    
    
    
    
  </entry>
  
</feed>
